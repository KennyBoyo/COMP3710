{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jun Khai\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (None)/charset_normalizer (2.1.1) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn\n",
    "import torch\n",
    "import torchsummary\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size:\n",
      "n_samples: 1288\n",
      "n_features: 1850\n",
      "n_classes: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapper(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        else:\n",
    "            return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5006536  0.53333336 0.5372549  ... 0.8705883  0.8928105  0.88366014]\n",
      " [0.6535948  0.55947715 0.49019608 ... 0.12418301 0.1503268  0.16470589]\n",
      " [0.45751634 0.45882353 0.46797386 ... 0.12156863 0.11764706 0.10980392]\n",
      " ...\n",
      " [0.48496735 0.45228758 0.45620918 ... 0.39869285 0.39084968 0.52679735]\n",
      " [0.43398693 0.44575164 0.6052287  ... 0.20653595 0.19215687 0.22222222]\n",
      " [0.04444445 0.05490196 0.06143792 ... 0.64836603 0.71503264 0.83267975]]\n",
      "(966, 1850)\n",
      "(322, 1850)\n",
      "X_train shape: (966, 1, 50, 37)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24137433010>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from torch.nn import module\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "x_tr, x_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25, random_state=42)# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "# x_tr, x_ts, y_tr, y_ts = torch.from_numpy(x_tr), torch.from_numpy(x_ts), torch.from_numpy(y_tr), torch.from_numpy(y_ts)\n",
    "print(x_tr)\n",
    "#normalise\n",
    "x_tr = x_tr / 255.0\n",
    "x_ts = x_ts / 255.0\n",
    "print(x_tr.shape)\n",
    "print(x_ts.shape)\n",
    "x_tr = x_tr.reshape(-1, 1, h, w)\n",
    "x_ts = x_ts.reshape(-1, 1, h, w)\n",
    "\n",
    "print(\"X_train shape:\", x_tr.shape)\n",
    "plt.imshow(x_tr[0, 0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LFWCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LFWCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3,stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3,stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(26112, 100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(100,7)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        o = self(x)\n",
    "        l = torch.max(o, 1)[1]\n",
    "        return l\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        '''\n",
    "        Compute the model's accuracy on the given dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input tensor\n",
    "        y: ground truth values\n",
    "        '''\n",
    "        pred_cls = self.predict(x)\n",
    "        true_cls = torch.max(y, 1)[1]\n",
    "        return (pred_cls == true_cls).sum().float().item() / len(y)\n",
    "        \n",
    "    \n",
    "# class LFWCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 64, 3)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(64, 32, 3)\n",
    "#         self.conv3 = nn.Conv2d(32, 8, 2)\n",
    "#         self.fc1 = nn.Linear(120, 100)\n",
    "#         self.fc2 = nn.Linear(100, 50)\n",
    "#         self.fc3 = nn.Linear(50, 7)\n",
    "#         self.sm = nn.Softmax(1)\n",
    "#         # self.fc3 = nn.Linear(25, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         # x = self.sm(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, x, y, x_ts, y_ts, lossfunc, lr=0.1, momentum=0, batch_size=600, nepochs=10, loop_prepend=\"\"):\n",
    "    device = next(net.parameters()).device # check what device the net parameters are on\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    accList = []\n",
    "    testAccList = []\n",
    "    net.train()\n",
    "\n",
    "    # training loop\n",
    "    dataloader = DataLoader(DatasetWrapper(x, y), batch_size=batch_size, shuffle=True)\n",
    "    loop = tqdm(range(nepochs), ncols=110)\n",
    "    for i in loop: # for each epoch\n",
    "        \n",
    "        # Task: fill in your training code below and compute epoch_loss (the average loss on all batches in a epoch)\n",
    "        epoch_loss = 0\n",
    "        for (x_batch, y_batch) in dataloader: # for each mini-batch\n",
    "            # Get prediction from model and calculate loss\n",
    "            x_batch.to(device)\n",
    "            y_batch.to(device)\n",
    "            y_pred = net(x_batch)\n",
    "            loss = lossfunc(y_pred, y_batch)            \n",
    "            epoch_loss += loss\n",
    "            # Reset optimiser gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # pass\n",
    "        n_batches = len(dataloader)\n",
    "        epoch_loss /= n_batches\n",
    "        \n",
    "\n",
    "        # evaluate network performance\n",
    "        acc = test(net, x, y, batch_size=batch_size)\n",
    "        testAcc = test(net, x_ts, y_ts, batch_size=batch_size)\n",
    "        \n",
    "        accList.append(acc.item())\n",
    "        testAccList.append(testAcc.item())\n",
    "        \n",
    "\n",
    "        # show training progress\n",
    "        loop.set_description(loop_prepend)\n",
    "        loop.set_postfix(loss=\"%5.5f\" % (epoch_loss),\n",
    "                         train_acc=\"%.2f%%\" % (100*acc))\n",
    "    \n",
    "    return accList, testAccList\n",
    "\n",
    "# try running test(FashionMLP(), x_ts_mlp, y_ts_mlp, showerrors=True) to see what the code does\n",
    "def test(net, x, y, batch_size=600, showerrors=False):\n",
    "    with torch.no_grad(): # disable automatic gradient computation for efficiency\n",
    "        device = next(net.parameters()).device\n",
    "\n",
    "        pred_cls = []\n",
    "        # make predictions on mini-batches  \n",
    "        dataloader = DataLoader(DatasetWrapper(x), batch_size=batch_size, shuffle=False)\n",
    "        for x_batch in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            pred_cls.append(torch.max(net(x_batch), 1)[1].cpu())\n",
    "\n",
    "        # compute accuracy\n",
    "        pred_cls = torch.cat(pred_cls) # concat predictions on the mini-batches\n",
    "        true_cls = torch.max(y, 1)[1].cpu()\n",
    "        acc = (pred_cls == true_cls).sum().float() / len(y)\n",
    "\n",
    "        # show errors if required\n",
    "        if showerrors:\n",
    "            idx_errors = (pred_cls != true_cls)\n",
    "\n",
    "            x_errors = x[idx_errors][:10].cpu()\n",
    "            y_pred = pred_cls[idx_errors][:10].cpu().numpy()\n",
    "            y_true = true_cls[idx_errors][:10].cpu().numpy()\n",
    "\n",
    "        return acc        \n",
    "\n",
    "\n",
    "# def train(net, x, y, x_ts, y_ts, lossfunc, lr=0.1, momentum=0, batch_size=600, nepochs=10, loop_prepend=\"\"):\n",
    "#     device = next(net.parameters()).device # check what device the net parameters are on\n",
    "#     optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "#     accList = []\n",
    "#     testAccList = []\n",
    "\n",
    "#     # training loop\n",
    "#     dataloader = DataLoader(DatasetWrapper(x, y), batch_size=batch_size, shuffle=True)\n",
    "#     loop = tqdm(range(nepochs), ncols=110)\n",
    "#     for i in loop: # for each epoch\n",
    "#         epoch_loss = 0\n",
    "#         for (x_batch, y_batch) in dataloader: # for each mini-batch\n",
    "#             # print(\"x_batch\", x_batch.shape)\n",
    "#             # print(\"y_batch\", y_batch)\n",
    "#             # Get prediction from model and calculate loss\n",
    "#             x_batch.to(device)\n",
    "#             y_batch.to(device)\n",
    "#             y_pred = net(x_batch)\n",
    "#             # print(\"ypred\", y_pred[0][0])\n",
    "#             # print(y_batch)\n",
    "#             # print(y_pred)\n",
    "#             loss = lossfunc(y_pred, y_batch)\n",
    "#             print(loss)\n",
    "#             # print(loss)\n",
    "#             epoch_loss += loss.item()\n",
    "#             # Reset optimiser gradients\n",
    "#             optimizer.zero_grad()\n",
    "#             # Backpropagation\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#         n_batches = len(dataloader)\n",
    "#         epoch_loss /= n_batches\n",
    "        \n",
    "\n",
    "#         # evaluate network performance\n",
    "#         acc = test(net, x, y, batch_size=batch_size)\n",
    "#         testAcc = test(net, x_ts, y_ts, batch_size=batch_size)\n",
    "        \n",
    "#         accList.append(acc.item())\n",
    "#         testAccList.append(testAcc.item())\n",
    "        \n",
    "\n",
    "#         # show training progress\n",
    "#         loop.set_description(loop_prepend)\n",
    "#         loop.set_postfix(loss=\"%5.5f\" % (epoch_loss),\n",
    "#                          train_acc=\"%.2f%%\" % (100*acc))\n",
    "    \n",
    "#     return accList, testAccList\n",
    "\n",
    "# def test(net, x, y, batch_size=600, showerrors=False):\n",
    "#     with torch.no_grad(): # disable automatic gradient computation for efficiency\n",
    "#         device = next(net.parameters()).device\n",
    "\n",
    "#         pred_cls = []\n",
    "#         # make predictions on mini-batches  \n",
    "#         dataloader = DataLoader(DatasetWrapper(x), batch_size=batch_size, shuffle=False)\n",
    "#         for x_batch in dataloader:\n",
    "#             x_batch = x_batch.to(device)\n",
    "#             pred_cls.append(torch.max(net(x_batch), 1)[1].cpu())\n",
    "\n",
    "#         # compute accuracy\n",
    "#         pred_cls = torch.cat(pred_cls) # concat predictions on the mini-batches\n",
    "#         true_cls = torch.max(y, 1)[1].cpu()\n",
    "#         acc = (pred_cls == true_cls).sum().float() / len(y)\n",
    "\n",
    "#         # show errors if required\n",
    "#         if showerrors:\n",
    "#             idx_errors = (pred_cls != true_cls)\n",
    "\n",
    "#             x_errors = x[idx_errors][:10].cpu()\n",
    "#             y_pred = pred_cls[idx_errors][:10].cpu().numpy()\n",
    "#             y_true = true_cls[idx_errors][:10].cpu().numpy()\n",
    "\n",
    "#         return acc        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun Khai\\AppData\\Local\\Temp\\ipykernel_26608\\3927534834.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_tr, x_ts, y_tr, y_ts = torch.tensor(x_tr).to(device), torch.tensor(x_ts).to(device), torch.tensor(int2onehot(y_tr)).to(device), torch.tensor(int2onehot(y_ts)).to(device)\n"
     ]
    }
   ],
   "source": [
    "def int2onehot(y):\n",
    "    onehot = torch.zeros(len(y), y.max()+1)\n",
    "    onehot[torch.arange(len(y)), y] = 1\n",
    "    return onehot.float()\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "x_tr, x_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "x_tr = x_tr.reshape(-1, 1, h, w)\n",
    "x_ts = x_ts.reshape(-1, 1, h, w)\n",
    "x_tr, x_ts, y_tr, y_ts = torch.tensor(x_tr).to(device), torch.tensor(x_ts).to(device), torch.tensor(int2onehot(y_tr)).to(device), torch.tensor(int2onehot(y_ts)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x26112 and 28800x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m LFWCNN()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m lossfunc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_prepend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m test(net, x_ts, y_ts, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, x, y, x_ts, y_ts, lossfunc, lr, momentum, batch_size, nepochs, loop_prepend)\u001b[0m\n\u001b[0;32m     19\u001b[0m x_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 21\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossfunc(y_pred, y_batch)            \n\u001b[0;32m     23\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[30], line 31\u001b[0m, in \u001b[0;36mLFWCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m     30\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(out)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x26112 and 28800x100)"
     ]
    }
   ],
   "source": [
    "net = LFWCNN().to(device)\n",
    "\n",
    "lossfunc = nn.CrossEntropyLoss()\n",
    "train(net, x_tr, y_tr, x_ts, y_ts, lossfunc, lr=0.01, momentum=0, batch_size=32, nepochs=500, loop_prepend=\"\")\n",
    "test(net, x_ts, y_ts, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.9677e-01, -1.4133e-01,  2.0224e-01, -5.6052e-01, -1.4909e-01,\n",
      "         1.2156e-01, -1.3743e-01,  7.2361e-01, -2.7691e-01,  4.3781e-01,\n",
      "        -2.3370e-01, -1.7290e-01,  1.3567e-01,  5.4978e-03, -9.9035e-02,\n",
      "         1.6483e-01,  2.4506e-01, -1.8207e-01, -6.2783e-01, -1.9998e-03,\n",
      "        -3.2500e-01, -1.8815e-01,  3.7839e-01, -2.4292e-01,  5.1796e-01,\n",
      "        -3.6339e-01, -5.5861e-01, -1.3992e-01, -2.9804e-01, -5.7278e-02,\n",
      "        -8.1512e-01, -3.5655e-04,  3.6810e-01,  4.6214e-01, -2.4316e-01,\n",
      "        -1.0283e+00, -3.0585e-01, -2.9224e-01,  3.6787e-01, -4.1604e-02,\n",
      "        -4.2684e-01, -2.0470e-01, -1.2756e-01,  5.2632e-02,  3.0202e-01,\n",
      "         2.9621e-01, -5.4912e-01,  2.6923e-01, -3.2179e-01, -3.1334e-02,\n",
      "        -2.7307e-01,  1.2755e-01,  1.5015e-01, -1.2153e-01,  6.1761e-01,\n",
      "        -3.1481e-01, -2.6293e-01, -3.0579e-02, -1.2202e-01, -2.2435e-01,\n",
      "        -4.9439e-02, -2.8698e-01, -1.4986e-01, -3.1821e-01], device='cuda:0')\n",
      "Model No. of Parameters: 2899911\n",
      "LFWCNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=28800, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i, param in enumerate(net.parameters()):\n",
    "    if i == 1:\n",
    "        print(param.data)\n",
    "        \n",
    "print(\"Model No. of Parameters:\", sum([param.nelement() for param in net.parameters()]))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./cifar10', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./cifar10', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c9dc99627a09bb05a8bd48a8dca3314e93c61f811f4ad65c484bbe198963cb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
